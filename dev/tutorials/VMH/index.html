<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Neural Graph Partial Differential Equations · NeuralGraphPDE.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://YichengDWu.github.io/NeuralGraphPDE.jl/tutorials/VMH/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralGraphPDE.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../graph_node/">Neural Graph Ordinary Differential Equations</a></li><li class="is-active"><a class="tocitem" href>Neural Graph Partial Differential Equations</a><ul class="internal"><li><a class="tocitem" href="#Load-the-packages"><span>Load the packages</span></a></li><li><a class="tocitem" href="#Load-data"><span>Load data</span></a></li><li><a class="tocitem" href="#Utilities-function"><span>Utilities function</span></a></li><li><a class="tocitem" href="#Model"><span>Model</span></a></li><li><a class="tocitem" href="#Optimiser"><span>Optimiser</span></a></li><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Train-the-model"><span>Train the model</span></a></li><li><a class="tocitem" href="#Expected-output"><span>Expected output</span></a></li></ul></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../../api/layers/">Layers</a></li><li><a class="tocitem" href="../../api/utilities/">Utilities</a></li></ul></li><li><a class="tocitem" href="../../devdoc/">Developer Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Neural Graph Partial Differential Equations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Neural Graph Partial Differential Equations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/YichengDWu/NeuralGraphPDE.jl/blob/main/docs/src/tutorials/VMH.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Graph-Partial-Differential-Equations"><a class="docs-heading-anchor" href="#Neural-Graph-Partial-Differential-Equations">Neural Graph Partial Differential Equations</a><a id="Neural-Graph-Partial-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Graph-Partial-Differential-Equations" title="Permalink"></a></h1><p>This tutorial is adapted from the paper <a href="https://github.com/yakovlev31/graphpdes_experiments/blob/master/convdiff/train.py">LEARNING CONTINUOUS-TIME PDES FROM SPARSE DATA WITH GRAPH NEURAL NETWORKS</a>. We will use <a href="../../api/layers/#NeuralGraphPDE.VMHConv"><code>VMHConv</code></a> to learn the dynamics of the convection-diffusion equation defined as</p><p class="math-container">\[\frac{\partial u(x, y, t)}{\partial t}=0.25 \nabla^{2} u(x, y, t)-\mathbf{v} \cdot \nabla u(x, y, t).\]</p><p>Specifically, we will learn the operator from the inital condition to the solution on the given temporal and spatial domain.</p><h2 id="Load-the-packages"><a class="docs-heading-anchor" href="#Load-the-packages">Load the packages</a><a id="Load-the-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-the-packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using DataDeps, MLUtils, Fetch
using NeuralGraphPDE, Lux, Optimisers, Random
using CUDA, JLD2
using SciMLSensitivity, DifferentialEquations
using Zygote
using Flux.Losses: mse
import Lux: initialparameters, initialstates
using NNlib
using DiffEqFlux: NeuralODE</code></pre><h2 id="Load-data"><a class="docs-heading-anchor" href="#Load-data">Load data</a><a id="Load-data-1"></a><a class="docs-heading-anchor-permalink" href="#Load-data" title="Permalink"></a></h2><pre><code class="language-julia hljs">function register_convdiff()
    return register(DataDep(&quot;Convection_Diffusion_Equation&quot;,
                            &quot;&quot;&quot;
                            Convection-Diffusion equation dataset from
                            [Learning continuous-time PDEs from sparse data with graph neural networks](https://github.com/yakovlev31/graphpdes_experiments)
                            &quot;&quot;&quot;,
                            &quot;https://drive.google.com/file/d/1oyatNeLizoO5co2ZVXIwZmWjJ046E9j6/view?usp=sharing&quot;;
                            fetch_method=gdownload))
end

register_convdiff()

function get_data()
    data = load(joinpath(datadep&quot;Convection_Diffusion_Equation&quot;, &quot;convdiff_n3000.jld2&quot;))

    train_data = (data[&quot;gs_train&quot;], data[&quot;u_train&quot;])
    test_data = (data[&quot;gs_test&quot;], data[&quot;u_test&quot;])
    return train_data, test_data, data[&quot;dt_train&quot;], data[&quot;dt_test&quot;], data[&quot;tspan_train&quot;],
           data[&quot;tspan_test&quot;]
end

train_data, test_data, dt_train, dt_test, tspan_train, tspan_test = get_data()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}[GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472)  …  GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472)], [0.27177718 0.23937638 … 0.42235315 0.41736898; 0.21528548 0.1956899 … 0.43383837 0.42533794; … ; 0.34221563 0.3904251 … 0.5707478 0.55885684; 0.39733648 0.44065434 … 0.5460154 0.5373213;;; 0.6088588 0.6091988 … 0.6238272 0.6237031; 0.6620063 0.65413135 … 0.62142694 0.6214922; … ; 0.68301886 0.62650585 … 0.5867061 0.61041445; 0.15491875 0.18275763 … 0.50021714 0.49130455;;; 0.53923243 0.57713723 … 0.61369747 0.60968894; 0.4529313 0.50201744 … 0.61193085 0.6085183; … ; 0.86270213 0.86968255 … 0.34141946 0.33516222; 0.8416236 0.81410044 … 0.6529443 0.6491246;;; … ;;; 0.53005123 0.5142571 … 0.53653616 0.52344817; 0.5106739 0.49997246 … 0.5831849 0.5668304; … ; 0.5212612 0.5202565 … 0.41233268 0.40637177; 0.54890347 0.5561268 … 0.4212825 0.4196272;;; 0.7143447 0.68895495 … 0.4071307 0.40816742; 0.6922809 0.66664165 … 0.4212077 0.4225582; … ; 0.55738837 0.5547057 … 0.32667074 0.33436233; 0.56979585 0.5690957 … 0.34755823 0.34747365;;; 0.570878 0.5683722 … 0.51076573 0.50354195; 0.5547831 0.55492985 … 0.48554698 0.47597125; … ; 0.47069466 0.51299846 … 0.37722296 0.36119726; 0.4099427 0.44195706 … 0.405135 0.38460922]), (GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}[GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472)  …  GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472), GNNGraph(2912, 17472)], [0.5149223 0.55170023 … 0.6248634 0.62268436; 0.43925944 0.47944194 … 0.6214465 0.62069637; … ; 0.56612545 0.57916313 … 0.6281285 0.6321008; 0.4334923 0.4407123 … 0.58638793 0.58987784;;; 0.22460508 0.26920357 … 0.50755596 0.5097602; 0.19417556 0.23524378 … 0.50171965 0.50386363; … ; 0.553888 0.5182864 … 0.36891004 0.36974698; 0.25890443 0.26049393 … 0.44759774 0.4444001;;; 0.6038077 0.5779638 … 0.525803 0.5221338; 0.6054157 0.580127 … 0.52578473 0.52208436; … ; 0.83374864 0.83018935 … 0.4135459 0.41875854; 0.25839168 0.25704578 … 0.3929463 0.39229473;;; … ;;; 0.5009132 0.491767 … 0.4384263 0.43659583; 0.47495937 0.47529647 … 0.43956596 0.43832278; … ; 0.6088848 0.5462766 … 0.38454705 0.38363633; 0.23869595 0.27031878 … 0.4214499 0.41845664;;; 0.41262475 0.43334606 … 0.50877845 0.5069962; 0.2918109 0.31498253 … 0.5289032 0.52331084; … ; 0.49011603 0.47694343 … 0.46114618 0.4575798; 0.5308936 0.5296045 … 0.4887325 0.48860204;;; 0.042765517 0.07461907 … 0.4805961 0.47712252; 0.06063889 0.05464858 … 0.48565334 0.48300737; … ; 0.28119192 0.2442868 … 0.49623835 0.49621361; 0.5339831 0.55114985 … 0.36704236 0.3728751]), 0.01f0, 0.01f0, (0.0f0, 0.2f0), (0.0f0, 0.6f0))</code></pre><p>The training data contrains 24 simulations on the time interval <span>$[0,0.2]$</span>. Simulations are obeserved on different 2D grids with 3000 points. Neighbors for each node were selected by applying Delaunay triangulation to the measurement positions. Two nodes were considered to be neighbors if they lie on the same edge of at least one triangle.</p><h2 id="Utilities-function"><a class="docs-heading-anchor" href="#Utilities-function">Utilities function</a><a id="Utilities-function-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities-function" title="Permalink"></a></h2><pre><code class="language-julia hljs">function diffeqsol_to_array(x::ODESolution{T, N, &lt;:AbstractVector{&lt;:CuArray}}) where {T, N}
    return gpu(x)
end

diffeqsol_to_array(x::ODESolution) = Array(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">diffeqsol_to_array (generic function with 2 methods)</code></pre><h2 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h2><p>We will use only one message passing layer. The layer will have the following structure:</p><pre><code class="language-julia hljs">initialparameters(rng::AbstractRNG, node::NeuralODE) = initialparameters(rng, node.model)
initialstates(rng::AbstractRNG, node::NeuralODE) = initialstates(rng, node.model)

act = tanh
nhidden = 60
nout = 40

ϕ = Chain(Dense(4 =&gt; nhidden, act), Dense(nhidden =&gt; nhidden, act),
          Dense(nhidden =&gt; nhidden, act), Dense(nhidden =&gt; nout))

γ = Chain(Dense(nout + 1 =&gt; nhidden, act), Dense(nhidden =&gt; nhidden, act),
          Dense(nhidden =&gt; nhidden, act), Dense(nhidden =&gt; 1))

gnn = VMHConv(ϕ, γ)

node = NeuralODE(gnn, tspan_train, Tsit5(); saveat=dt_train, reltol=1e-9, abstol=1e-3)

model = Chain(node, diffeqsol_to_array)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = NeuralODE(),              # 19_961 parameters
    layer_2 = WrappedFunction(diffeqsol_to_array),
)         # Total: 19_961 parameters,
          #        plus 0 states, summarysize 784 bytes.</code></pre><h2 id="Optimiser"><a class="docs-heading-anchor" href="#Optimiser">Optimiser</a><a id="Optimiser-1"></a><a class="docs-heading-anchor-permalink" href="#Optimiser" title="Permalink"></a></h2><p>Since we only have 24 samples, we will use the <code>Rprop</code> optimiser.</p><pre><code class="language-julia hljs">opt = Rprop(1.0f-6, (5.0f-1, 1.2f0), (1.0f-8, 10.0f0))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimisers.Rprop{Float32}(1.0f-6, (0.5f0, 1.2f0), (1.0f-8, 10.0f0))</code></pre><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><p>We will use the <code>mse</code> loss function.</p><pre><code class="language-julia hljs">function loss(x, y, ps, st)
    ŷ, st = model(x, ps, st)
    l = mse(ŷ, y)
    return l
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss (generic function with 1 method)</code></pre><h2 id="Train-the-model"><a class="docs-heading-anchor" href="#Train-the-model">Train the model</a><a id="Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-model" title="Permalink"></a></h2><p>The solution data has the shape <code>(space_points , time_points, num_samples)</code>. We will first permute the last two dimensions, resulting in the shape <code>(space_points , num_samples, time_points)</code>. Then we flatten the first two dimensions, <code>(1, space_points * num_samples, time_points)</code>, and use the initial condition as the input to the model. The output of the model will be of size <code>(1, space_points * time_points, num_samples)</code>.</p><pre><code class="language-julia hljs">mydevice = CUDA.functional() ? gpu : cpu
train_loader = DataLoader(train_data; batchsize=24, shuffle=true)

rng = Random.default_rng()
Random.seed!(rng, 0)

function train()
    ps, st = Lux.setup(rng, model)
    ps = Lux.ComponentArray(ps) |&gt; mydevice
    st = st |&gt; mydevice
    st_opt = Optimisers.setup(opt, ps)

    for i in 1:200
        for (g, u) in train_loader
            g = g |&gt; mydevice
            st = updategraph(st, g)
            u = u |&gt; mydevice
            u0 = reshape(u[:, 1, :], 1, :)
            ut = permutedims(u, (1, 3, 2))
            ut = reshape(ut, 1, g.num_nodes, :)

            l, back = pullback(p -&gt; loss(u0, ut, p, st), ps)
            ((i - 1) % 10 == 0) &amp;&amp; @info &quot;epoch $i | train loss = $l&quot;
            gs = back(one(l))[1]
            st_opt, ps = Optimisers.update(st_opt, ps, gs)
        end
    end
end

train()</code></pre><h2 id="Expected-output"><a class="docs-heading-anchor" href="#Expected-output">Expected output</a><a id="Expected-output-1"></a><a class="docs-heading-anchor-permalink" href="#Expected-output" title="Permalink"></a></h2><pre><code class="nohighlight hljs">[ Info: epoch 10 | train loss = 0.02720912251427  0.53685   0.425613  0.71604
[ Info: epoch 20 | train loss = 0.026874812
[ Info: epoch 30 | train loss = 0.025392009
[ Info: epoch 40 | train loss = 0.023239506
[ Info: epoch 50 | train loss = 0.010599495
[ Info: epoch 60 | train loss = 0.010421633
[ Info: epoch 70 | train loss = 0.0098072495
[ Info: epoch 80 | train loss = 0.008936066
[ Info: epoch 90 | train loss = 0.0063929264
[ Info: epoch 100 | train loss = 0.004207685
[ Info: epoch 110 | train loss = 0.0026181203
[ Info: epoch 120 | train loss = 0.0023022622
[ Info: epoch 130 | train loss = 0.0019534715
[ Info: epoch 140 | train loss = 0.0017379699
[ Info: epoch 150 | train loss = 0.0015728847
[ Info: epoch 160 | train loss = 0.0013444767
[ Info: epoch 170 | train loss = 0.0012353633
[ Info: epoch 180 | train loss = 0.0011409305
[ Info: epoch 190 | train loss = 0.0010424983
[ Info: epoch 200 | train loss = 0.0009809926</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../graph_node/">« Neural Graph Ordinary Differential Equations</a><a class="docs-footer-nextpage" href="../../api/layers/">Layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 14 November 2022 21:55">Monday 14 November 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
